Traceback (most recent call last):
  File "/work2/09753/ishdeshpa/frontera/ImageRestoration/Microsoft/Bringing-Old-Photos-Back-to-Life/Face_Detection/align_warp_back_multiple_dlib_HR.py", line 428, in <module>
    blended = blur_blending_cv2(warped_back, blended, backward_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work2/09753/ishdeshpa/frontera/ImageRestoration/Microsoft/Bringing-Old-Photos-Back-to-Life/Face_Detection/align_warp_back_multiple_dlib_HR.py", line 219, in blur_blending_cv2
    mask *= 255.0
numpy.core._exceptions._UFuncOutputCastingError: Cannot cast ufunc 'multiply' output from dtype('float64') to dtype('uint8') with casting rule 'same_kind'
/scratch1/09753/ishdeshpa/miniconda3/envs/gfpgan/lib/python3.11/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.
  warnings.warn(
/scratch1/09753/ishdeshpa/miniconda3/envs/gfpgan/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/scratch1/09753/ishdeshpa/miniconda3/envs/gfpgan/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
/scratch1/09753/ishdeshpa/miniconda3/envs/hat/lib/python3.11/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.
  warnings.warn(
2023-12-04 20:07:15,463 INFO: 
                ____                _       _____  ____
               / __ ) ____ _ _____ (_)_____/ ___/ / __ \
              / __  |/ __ `// ___// // ___/\__ \ / /_/ /
             / /_/ // /_/ /(__  )/ // /__ ___/ // _, _/
            /_____/ \__,_//____//_/ \___//____//_/ |_|
     ______                   __   __                 __      __
    / ____/____   ____   ____/ /  / /   __  __ _____ / /__   / /
   / / __ / __ \ / __ \ / __  /  / /   / / / // ___// //_/  / /
  / /_/ // /_/ // /_/ // /_/ /  / /___/ /_/ // /__ / /<    /_/
  \____/ \____/ \____/ \____/  /_____/\____/ \___//_/|_|  (_)
    
Version Information: 
	BasicSR: 1.3.4.9
	PyTorch: 2.1.1
	TorchVision: 0.16.1
2023-12-04 20:07:15,465 INFO: 
  name: _job_1_batch_2
  model_type: HATModel
  scale: 4
  num_gpu: 1
  manual_seed: 0
  tile:[
    tile_size: 256
    tile_pad: 32
  ]
  datasets:[
    test_1:[
      name: Set5
      type: SingleImageDataset
      dataroot_lq: /work2/09753/ishdeshpa/frontera/ImageRestoration/GFPGAN/results_job_1_batch_2/restored_imgs
      io_backend:[
        type: disk
      ]
      phase: test
      scale: 4
    ]
  ]
  network_g:[
    type: HAT
    upscale: 4
    in_chans: 3
    img_size: 64
    window_size: 16
    compress_ratio: 24
    squeeze_factor: 24
    conv_scale: 0.01
    overlap_ratio: 0.5
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 144
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
  ]
  path:[
    pretrain_network_g: /work2/09753/ishdeshpa/frontera/ImageRestoration/HAT/HAT-S_SRx4.pth
    strict_load_g: True
    param_key_g: params_ema
    results_root: /work2/09753/ishdeshpa/frontera/ImageRestoration/HAT/HAT/results/_job_1_batch_2
    log: /work2/09753/ishdeshpa/frontera/ImageRestoration/HAT/HAT/results/_job_1_batch_2
    visualization: /work2/09753/ishdeshpa/frontera/ImageRestoration/HAT/HAT/results/_job_1_batch_2/visualization
  ]
  val:[
    save_img: True
    suffix: None
  ]
  dist: False
  rank: 0
  world_size: 1
  auto_resume: False
  is_train: False

2023-12-04 20:07:15,467 INFO: Dataset [SingleImageDataset] - Set5 is built.
2023-12-04 20:07:15,467 INFO: Number of test images in Set5: 24
/scratch1/09753/ishdeshpa/miniconda3/envs/hat/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449183005/work/aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
2023-12-04 20:07:19,214 INFO: Network [HAT] is created.
2023-12-04 20:07:22,516 INFO: Network: HAT, with parameters: 9,621,183
2023-12-04 20:07:22,516 INFO: HAT(
  (conv_first): Conv2d(3, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (patch_embed): PatchEmbed(
    (norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
  )
  (patch_unembed): PatchUnEmbed()
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): RHAG(
      (residual_group): AttenBlocks(
        (blocks): ModuleList(
          (0): HAB(
            (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=144, out_features=432, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=144, out_features=144, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (conv_block): CAB(
              (cab): Sequential(
                (0): Conv2d(144, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(approximate='none')
                (2): Conv2d(6, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  (attention): Sequential(
                    (0): AdaptiveAvgPool2d(output_size=1)
                    (1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(inplace=True)
                    (3): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid()
                  )
                )
              )
            )
            (drop_path): Identity()
            (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=144, out_features=288, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=288, out_features=144, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1-5): 5 x HAB(
            (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=144, out_features=432, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=144, out_features=144, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (conv_block): CAB(
              (cab): Sequential(
                (0): Conv2d(144, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(approximate='none')
                (2): Conv2d(6, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  (attention): Sequential(
                    (0): AdaptiveAvgPool2d(output_size=1)
                    (1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(inplace=True)
                    (3): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid()
                  )
                )
              )
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=144, out_features=288, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=288, out_features=144, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (overlap_attn): OCAB(
          (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
          (qkv): Linear(in_features=144, out_features=432, bias=True)
          (unfold): Unfold(kernel_size=(24, 24), dilation=1, padding=4, stride=16)
          (softmax): Softmax(dim=-1)
          (proj): Linear(in_features=144, out_features=144, bias=True)
          (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=144, out_features=288, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=288, out_features=144, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (conv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (1-5): 5 x RHAG(
      (residual_group): AttenBlocks(
        (blocks): ModuleList(
          (0-5): 6 x HAB(
            (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=144, out_features=432, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=144, out_features=144, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (conv_block): CAB(
              (cab): Sequential(
                (0): Conv2d(144, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(approximate='none')
                (2): Conv2d(6, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  (attention): Sequential(
                    (0): AdaptiveAvgPool2d(output_size=1)
                    (1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(inplace=True)
                    (3): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid()
                  )
                )
              )
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=144, out_features=288, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=288, out_features=144, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (overlap_attn): OCAB(
          (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
          (qkv): Linear(in_features=144, out_features=432, bias=True)
          (unfold): Unfold(kernel_size=(24, 24), dilation=1, padding=4, stride=16)
          (softmax): Softmax(dim=-1)
          (proj): Linear(in_features=144, out_features=144, bias=True)
          (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=144, out_features=288, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=288, out_features=144, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (conv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
  )
  (norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
  (conv_after_body): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_before_upsample): Sequential(
    (0): Conv2d(144, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
  )
  (upsample): Upsample(
    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): PixelShuffle(upscale_factor=2)
    (2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): PixelShuffle(upscale_factor=2)
  )
  (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
2023-12-04 20:07:23,736 INFO: Loading HAT model from /work2/09753/ishdeshpa/frontera/ImageRestoration/HAT/HAT-S_SRx4.pth, with param key: [params_ema].
2023-12-04 20:07:23,899 INFO: Model [HATModel] is created.
2023-12-04 20:07:23,900 INFO: Testing Set5...
